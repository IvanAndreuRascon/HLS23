<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Bimodal Speech Perception in Second Language Learning: The Power of Visual Cues</title>
    <meta charset="utf-8" />
    <meta name="author" content="Ivan Andreu Rascon" />
    <meta name="date" content="2023-05-07" />
    <script src="libs/header-attrs-2.20/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/rutgers-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/rutgers.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Bimodal Speech Perception in Second Language Learning: The Power of Visual Cues
]
.subtitle[
## Bimodal Speech Production in L2 Learners
]
.author[
### Ivan Andreu Rascon
]
.institute[
### Rutgers University
]
.date[
### 2023-05-07
]

---




# How do we perceive language?

.pull-left[
&lt;img src="figs/eye1.jpg" width="100%" /&gt;
]

.pull-right[
- Understanding language is a complex process that involves bringing together information from various sources, including the sound of speech, our knowledge of language structure, and context. 

- Extensive literature in L1 visual perception (O’Neill, 1954; Ross et al., 2007; Sumby &amp; Pollack, 1954).

- Majority of studies based on L1 and participants with disabilities.

- Scarce, but promising research in SLA bimodal speech perception suggests that can have beneficial implications for second language learning (Brancazio &amp; Miller, 2005; Hazan et al., 2006)
]
---
# This study. 

### Experiment 1: **Identification of Spanish vowels.**

### Experiment 2: **L2 identification of monophthong-diphthong alternations.**

### Experiment 3: **Modality effects on perception of Spanish resyllabification.**

---

# Background: Speech perception. 

--

## Motor theory (Liberman &amp; Mattingly, 1985) 
- Speaker’s ability to identify the specific motor commands that underlie the speaker’s articulatory movements. Eg: “tongue backing,” “lip rounding,” and “jaw raising.”
- McGurk effect, (McGurk &amp; MacDonald, 1976). 
 - Fused perception of language. 

--

## The Direct Realist Theory (Fowler, 1986).
- Similar to the MT, speech perception is rooted in the analysis of articulatory events, rather than acoustic events.
- Process that relies on the integration of multiple sources of information,

#### Effect of visual cues? How do humans perceive sound in noisy environments? 

---

# Background: Second language speech perception

--

## The Speech Learning Model (revised) (Flege &amp; Bohn, 2021). 

- Growth and development of phonetic categories in second-language learning. 
- Considers the integration of various sources to perceive language. 

--

## The Perception Assimilation Model (PAM-L2) (Best et al, 2007). 

- Learners assimilate L2 sounds to the most similar L1 sounds.
  - We can make predictions based on phonemic inventories. 



- Having more (vowel) phonemes, does not imply better perception. Escudero &amp; Williams (2012), Gordon (2008),Inceoglu (2022). 
  
---
# Background: Second language speech and visual perception. 

## Within the PAM-L2 precepts

- Hazan et al. (2006) identified three types of visemes 
  - (1) those present in both L1 and L2, 
  - (2) those unique to L2
  - (3) those that differ between L1 and L2. 
  
A viseme was defined as: 

*The visual representation of speech sounds seen in the movements and shapes of the mouth and lips. *

- **L1 phonemes and visemes affect L2 perception**
  - Categories (2) and (3), their L2 acquisition might be hindered
---

# Background: Second language speech and visual perception.  

- Fenwick et al. (2017) phonemes and visemes that are similar in the L1 are easier to perceive and categorize. 

- Desroches et al. (2022) bilinguals have a flexible language processing system. 
  - **Cross-language connections** between lexical representations and phonology.
  - Bilinguals automatically activate lexical options from both languages when looking at pictures. 
  - Bilinguals can execute top-down cross-language connections. 

- Marian et al. (2018), l2 experience the McGurk effect more often than monolinguals. 
  - Bilinguals possess dual visemic and phonemic inventories, expanding their range of linguistic possibilities.
  
---

# Research Questions  

1.	Are second-language English learners of Spanish capable of identifying vowels by visually observing the respective articulation in natural speech? Does a correlation exist between their accuracy and language proficiency?

2.	To what extent does incongruity between auditory and visual inputs impede the processing of auditory information? Does the proficiency level of a learner influence their ability to effectively process and prioritize between two competing inputs?

---

# Experiment 1: Identification of Spanish vowels

.pull-left[
- Input was recorded in a sound attenuated booth. 
- Video was later trimmed from the septal cartilage to the mandible mentalis muscle
- Target words. 
- Consonants /p, t, k, b, d, g, f, j, s and θ/. 
- With /a, e, i, o u/.
  - Created bi-syllable words.
- All possible answers were provided 
  - Audio only     50 items
  - Visual only    50 items
  - Audiovisual    50 items
  - Mismatch       50 items
- **Total**     200 items 
]
--
.pull-right[
&lt;img src="figs/vowel_mismatch_decision.jpg" width="100%" /&gt;
##### Procedure to create vowel mismatch condition
]
---
.pull-left[
## Procedure 
  
1. LexTALE
2. Experiment 1, (followed by 2 and 3)
 Input was randomized 
3. Background questionary

## Participants 
&lt;img src="figs/s1part.png" width="80%" /&gt;
]
--
.pull-right[
## Statistical analysis

- This experiment utilized a Bayesian multinomial logistic regression to assess the probability of an observation being identified as one of the five Spanish vowels (/a/, /e/, /i/, /o/, /u/).
- The responses were modeled as a function of condition and language proficiency. Likelihood of identifying a specific vowel in: 
  - audio, 
  - visual-only 
  - audiovisual
  - audiovisual mismatch
  ]

---
# Results 
.center[
&lt;img src="figs/s1a.png" width="40%" /&gt;
]

---
# Results 
.pull-left[
&lt;img src="figs/s1e.png" width="70%" /&gt;
]
--
.pull-right[
&lt;img src="figs/s1i.png" width="75%" /&gt;
]

---
# Results 
.pull-left[
&lt;img src="figs/s1o.png" width="70%" /&gt;
]
--
.pull-right[
&lt;img src="figs/s1u.png" width="73%" /&gt;
]

---
&lt;img src="figs/s1_prob.png" width="100%" /&gt;

.center[
&lt;img src="figs/vowel_mismatch_decision.jpg" width="40%" /&gt;
]

---
# Results: LexTALE

.center[
&lt;img src="figs/s1audio.png" width="95%" /&gt;
]
---
# Results: LexTALE
.center[
&lt;img src="figs/s1av.png" width="100%" /&gt;
]

---
# Results: LexTALE
.center[
&lt;img src="figs/s1visual.png" width="95%" /&gt;
]
---
# Results: LexTALE
.center[
&lt;img src="figs/s1avmis.png" width="90%" /&gt;
]
---
.center[
&lt;img src="figs/s1_all.png" width="55%" /&gt;
]
---
# Experiment 2: L2 identification of monophthong-diphthong alternations.

.pull-left[
## Method 
- Same conditions and procedure as in Experiment 1.
- Real words were used
  - Pena vs Peina
  - Vale vs Baile
- The mismatch condition was established by exchanging both input stimuli.
- With a total of **fifty-six items**
]

.pull-right[
## Statistical analysis
-Logistic regression model to investigate the likelihood of correctly selecting the correct combination of vowels in relation to the participant’ language proficiency.


## Participants 
&lt;img src="figs/s2part.png" width="100%" /&gt;

]

---
# Results 
.center[
&lt;img src="figs/s2_all.png" width="100%" /&gt;
]
---
# Experiment 3: Spanish Resyllabification.

--
La**s** alas / La **s**alas 

Ve**n** aves / Ve **n**aves

- According Hualde (2005) would be pronounced identically due to the resyllabification process.

- Later contradicted by Michnowicz &amp; Kagan, (2016), Lahoz-Bengoechea &amp; Jiménez-Bravo, (2021) finding durational differences.
  - Shorter coda/s
 
- The only study performed in L2 learners was performed by Scarpace, (2017). 

  - Confounding results. 
  - Mixed Mexico and Colombia dialects
  
---
# Research Questions  

- 	To what extent can second language learners perceive duration differences associated with resyllabification? 
-    Does access to visual articulatory cues impact second language learners’ ability to perceive differences between canonical and resyllabified occurrences?

---
.pull-left[
## Methods and procedure 

- The stimuli was recorded in a meaningful carrier sentence, e.g., *las alas del pájaro son grandes* vs *la comida la salas.*

- Video trimmed following Experiments 1 and 2 methodology. 

## Participants 

&lt;img src="figs/s3part.png" width="100%" /&gt;
]

.pull-right[
## Statistical analysis
-Logistic regression model to investigate the likelihood of correctly selecting the correct combination of syllabes in relation to the participant’ language proficiency.
]

---
# Results

.center[
&lt;img src="figs/s3consonante.png" width="130%" /&gt;
]
---
# Results

.center[
&lt;img src="figs/s3lexTALE.png" width="130%" /&gt;
]
---

# Discussion.

- The study explored the role of visual cues, such as lip and facial movements, in helping English speakers learn Spanish as a second language.

- Experiments 1/2/3 showed that as language proficiency increases, learners can rely more on visual cues, improving speech perception and reducing misidentification (Peelle &amp; Sommers, 2015; Tye-Murray et al., 2007).
  - Combination of auditory and visual information facilitates L2 speech perception.


- The results support both the (PAM-L2) and the (SLM).
  - Difficulty in distinguishing between the /e-i/ and /u-o/ vowel pairs.
  - New categories (phonemes and visemes) are created as proficiency advances. 
--

- Participants in Experiment 2 and 3 might have experienced intersection density (Tye-Murray et al. 2007). 

  - Higher proficiency students have more lexical options available.

  - Marian et al. (2018), l2 experience the McGurk effect more often than monolinguals.

--


- Experiment 3 aligns with Michnowicz &amp; Kagan, (2016), Lahoz-Bengoechea &amp; Jiménez-Bravo, (2021). 

  - Canonical and resyllabified have durational diferences. 

---
# Conclusion

- Experiments 1, 2, and 3 had different theoretical approaches, and therefore showed different performances across participants.

- L2 speakers beneficent from bimodal perception. 

- Learners develop new categories (phonemes and visemes) as proficiency advances. 

- The scope of this study can be extrapolated to the importance of integrating visual cues in language instruction to improve phonetic discrimination, particularly for learners with developing linguistic abilities.
  - Specifically in online language courses

---
# Questions?


---
# References


Best, C. T., Tyler, M., Bohn, O., &amp; Munro, M. (2007). Nonnative and second-language speech
perception. Language Experience in Second Language Speech Learning, 13–34.


Bradlow, A. R., &amp; Bent, T. (2002). The clear speech effect for non-native listeners. The Journal of the Acoustical Society of America, 112(1), 272–284.


Devlin, J. T., &amp; Aydelott, J. (2009). Speech perception: Motoric contributions versus the motor theory. Current Biology, 19(5), R198–R200.


Elvin, J., Escudero, P., &amp; Vasiliev, P. (2014). Spanish is better than english for discriminating portuguese vowels: Acoustic similarity versus vowel inventory size. Frontiers in Psychology, 5, 1188.


Escudero, P., &amp; Williams, D. (2012). Native dialect influences second-language vowel perception: Peruvian versus iberian spanish learners of dutch. The Journal of the Acoustical Society of America, 131(5), EL406–EL412.


Fenwick, S. E., Best, C. T., Davis, C., &amp; Tyler, M. D. (2017). The influence of auditory-visual speech and clear speech on cross-language perceptual assimilation. Speech Communication, 92, 114–124.

---
# References

Fisher, C. G. (1968). Confusions among visually perceived consonants. Journal of Speech and Hearing Research, 11(4), 796–804.


Flege, J. E., &amp; Bohn, O.-S. (2021). The revised speech learning model (SLM-r). Second Language Speech Learning: Theoretical and Empirical Progress, 10(9781108886901.002).


Fowler, C. A. (1986). An event approach to the study of speech perception from a direct–realist perspective. Journal of Phonetics, 14(1), 3–28.

Gordon, L. S. (2008). Factors affecting english speakers’ perception of L2 spanish vowels. Georgetown University.


Hazan, V., Sennema, A., Faulkner, A., Ortega-Llebaria, M., Iba, M., &amp; Chung, H. (2006). The use of visual cues in the perception of non-native consonant contrasts. The Journal of the Acoustical Society of America, 119(3), 1740–1751.
---
# References

Hualde, J. I. (2005). The sounds of spanish with audio CD. Cambridge University Press.


Hualde, J. I. (2013). Los sonidos del español: Spanish language edition. Cambridge University Press.


Inceoglu, S. (2022). Language experience and subjective word familiarity on the multimodal perception of non-native vowels. Language and Speech, 65(1), 173–192.


Lahoz-Bengoechea, J. M., &amp; Jiménez-Bravo, M. (2021). Spoken word boundary detection in ambiguous resyllabification contexts in spanish.


Lemhöfer, K., &amp; Broersma, M. (2012). Introducing LexTALE: A quick and valid lexical test for advanced learners of english. Behavior Research Methods, 44, 325–343.


Leonte, A., Colzato, L. S., Steenbergen, L., Hommel, B., &amp; Akyürek, E. G. (2018). Supplementation of gamma-aminobutyric acid (GABA) affects temporal, but not spatial visual attention. Brain and Cognition, 120, 8–16.


Levy, E. S., &amp; Strange, W. (2008). Perception of french vowels by american english adults with and without french language experience. Journal of Phonetics, 36(1), 141–157.
---
# References




    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
